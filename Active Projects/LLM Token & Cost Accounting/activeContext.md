# Active Context: LLM Token & Cost Accounting

**Last Updated:** 2026-02-02 11:24 EST

## Problem

No visibility into AI infrastructure costs. Need to track:
- How many tokens used daily/weekly/quarterly?
- Cost per LLM provider
- Cost trends over time
- Ability to make informed model choice decisions

Currently operating blind on costs.

## Approach

1. **API-level logging** â€” Capture token counts from Claude, OpenAI responses
2. **Daily aggregation** â€” Roll up into daily note section (ðŸ’° Token Usage)
3. **Weekly rollup** â€” 7-day summary in weekly note
4. **Quarterly reporting** â€” Trend analysis + budget comparison in quarterly note

Simple, consistent reporting across all time scales.

## Architecture

```
API Call â†’ Log Token Count â†’ Daily Aggregation â†’ Daily Note
                                   â†“
                            Weekly Aggregation â†’ Weekly Note
                                   â†“
                           Quarterly Aggregation â†’ Quarterly Note
```

## Blockers

- None â€” can start immediately
- No dependency on other projects
- Can integrate with existing daily note structure

## Decisions

None yet. Still in research phase.

## Next Steps

1. List all LLM APIs currently in use (Claude, OpenAI, etc.)
2. Understand what token data each API returns
3. Design logging schema (what to capture)
4. Decide storage format (JSON, CSV, database)
5. Build aggregation scripts
6. Test with daily notes integration
